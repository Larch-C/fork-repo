name: Batch Fork Repositories

on:
  workflow_dispatch:
    inputs:
      dry_run:
        description: 'æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼'
        required: false
        default: 'false'
        type: boolean
      download_latest:
        description: 'æ˜¯å¦ä¸‹è½½æœ€æ–°çš„ plugins.json'
        required: false
        default: 'true'
        type: boolean

jobs:
  batch-fork:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        
    - name: Install dependencies
      run: |
        npm install @octokit/rest
        npm install fs-extra
        
    - name: Batch Fork Script
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        DRY_RUN: ${{ inputs.dry_run }}
        DOWNLOAD_LATEST: ${{ inputs.download_latest }}
      run: |
        node -e "
        const { Octokit } = require('@octokit/rest');
        const fs = require('fs-extra');
        const path = require('path');
        const https = require('https');
        
        // åˆå§‹åŒ– Octokit
        const octokit = new Octokit({
          auth: process.env.GITHUB_TOKEN,
        });
        
        // ä¸‹è½½æœ€æ–°çš„ plugins.json
        async function downloadLatestPluginsJson() {
          return new Promise((resolve, reject) => {
            const url = 'https://raw.githubusercontent.com/AstrBotDevs/AstrBot_Plugins_Collection/main/plugins.json';
            
            https.get(url, (response) => {
              let data = '';
              
              response.on('data', (chunk) => {
                data += chunk;
              });
              
              response.on('end', () => {
                try {
                  const pluginsJson = JSON.parse(data);
                  resolve(pluginsJson);
                } catch (error) {
                  reject(new Error(\`Failed to parse plugins.json: \${error.message}\`));
                }
              });
            }).on('error', (error) => {
              reject(new Error(\`Failed to download plugins.json: \${error.message}\`));
            });
          });
        }
        
        // è·å–å½“å‰ç”¨æˆ·ä¿¡æ¯
        async function getCurrentUser() {
          const { data: user } = await octokit.rest.users.getAuthenticated();
          return user.login;
        }
        
        // ä» AstrBot plugins.json æå–ä»“åº“åœ°å€
        function extractRepoUrls(pluginJson) {
          const repoUrls = [];
          
          // AstrBot plugins.json ç»“æ„: { "æ’ä»¶å": { "repo": "githubåœ°å€", "desc": "æè¿°", "author": "ä½œè€…", ... } }
          for (const pluginName in pluginJson) {
            const pluginInfo = pluginJson[pluginName];
            
            if (pluginInfo && typeof pluginInfo === 'object' && pluginInfo.repo) {
              let repoUrl = pluginInfo.repo;
              
              // å¤„ç†ä¸åŒæ ¼å¼çš„ä»“åº“åœ°å€
              if (typeof repoUrl === 'string') {
                // æå– GitHub ä»“åº“è·¯å¾„
                let repoPath = null;
                
                // å¤„ç†å®Œæ•´çš„ GitHub URL
                if (repoUrl.includes('github.com')) {
                  const match = repoUrl.match(/github\.com\/([^\/]+\/[^\/]+)/);
                  if (match) {
                    repoPath = match[1].replace(/\.git$/, '').replace(/\/$/, '');
                  }
                }
                // å¤„ç†ç®€çŸ­æ ¼å¼ owner/repo
                else if (repoUrl.match(/^[^\/]+\/[^\/]+$/)) {
                  repoPath = repoUrl;
                }
                
                if (repoPath) {
                  repoUrls.push({
                    name: pluginName,
                    repo: repoPath,
                    description: pluginInfo.desc || '',
                    author: pluginInfo.author || ''
                  });
                }
              }
            }
          }
          
          // å»é‡ï¼ˆåŸºäº repo å­—æ®µï¼‰
          const uniqueRepos = [];
          const seenRepos = new Set();
          
          for (const item of repoUrls) {
            if (!seenRepos.has(item.repo)) {
              seenRepos.add(item.repo);
              uniqueRepos.push(item);
            }
          }
          
          return uniqueRepos;
        }
        
        // æ£€æŸ¥ä»“åº“æ˜¯å¦å·²ç»forkè¿‡
        async function checkIfForked(owner, repo, currentUser) {
          try {
            const { data: fork } = await octokit.rest.repos.get({
              owner: currentUser,
              repo: repo,
            });
            
            // æ£€æŸ¥æ˜¯å¦ä¸ºforkä¸”parentæŒ‡å‘ç›®æ ‡ä»“åº“
            if (fork.fork && fork.parent) {
              return fork.parent.full_name === \`\${owner}/\${repo}\`;
            }
            return false;
          } catch (error) {
            if (error.status === 404) {
              return false;
            }
            throw error;
          }
        }
        
        // Forkä»“åº“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        async function forkRepository(owner, repo, currentUser, retries = 3) {
          console.log(\`ğŸ´ Forking \${owner}/\${repo}...\`);
          
          if (process.env.DRY_RUN === 'true') {
            console.log(\`[DRY RUN] Would fork \${owner}/\${repo}\`);
            return;
          }
          
          for (let i = 0; i < retries; i++) {
            try {
              await octokit.rest.repos.createFork({
                owner: owner,
                repo: repo,
              });
              console.log(\`âœ… Successfully forked \${owner}/\${repo}\`);
              return;
            } catch (error) {
              if (error.status === 403 && error.message.includes('rate limit')) {
                console.log(\`â³ Rate limit hit for \${owner}/\${repo}, waiting 60 seconds...\`);
                await delay(60000);
                continue;
              }
              
              if (error.status === 403 && error.message.includes('already exists')) {
                console.log(\`â„¹ï¸  Fork \${owner}/\${repo} already exists\`);
                return;
              }
              
              if (i === retries - 1) {
                console.error(\`âŒ Failed to fork \${owner}/\${repo} after \${retries} attempts: \${error.message}\`);
                throw error;
              }
              
              console.log(\`âš ï¸  Fork attempt \${i + 1} failed for \${owner}/\${repo}, retrying...\`);
              await delay(5000);
            }
          }
        }
        
        // åŒæ­¥forkä»“åº“ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        async function syncFork(owner, repo, currentUser, retries = 3) {
          console.log(\`ğŸ”„ Syncing fork \${currentUser}/\${repo}...\`);
          
          if (process.env.DRY_RUN === 'true') {
            console.log(\`[DRY RUN] Would sync fork \${currentUser}/\${repo}\`);
            return;
          }
          
          for (let i = 0; i < retries; i++) {
            try {
              // è·å–ä¸Šæ¸¸ä»“åº“çš„é»˜è®¤åˆ†æ”¯
              const { data: upstream } = await octokit.rest.repos.get({
                owner: owner,
                repo: repo,
              });
              
              // åŒæ­¥fork
              await octokit.rest.repos.mergeUpstream({
                owner: currentUser,
                repo: repo,
                branch: upstream.default_branch,
              });
              
              console.log(\`âœ… Successfully synced fork \${currentUser}/\${repo}\`);
              return;
            } catch (error) {
              if (error.status === 403 && error.message.includes('rate limit')) {
                console.log(\`â³ Rate limit hit for \${currentUser}/\${repo}, waiting 60 seconds...\`);
                await delay(60000);
                continue;
              }
              
              if (error.status === 409) {
                console.log(\`â„¹ï¸  Fork \${currentUser}/\${repo} is already up to date\`);
                return;
              }
              
              if (error.status === 422) {
                console.log(\`â„¹ï¸  No commits between \${currentUser}/\${repo} and upstream\`);
                return;
              }
              
              if (i === retries - 1) {
                console.error(\`âŒ Failed to sync fork \${currentUser}/\${repo} after \${retries} attempts: \${error.message}\`);
                throw error;
              }
              
              console.log(\`âš ï¸  Sync attempt \${i + 1} failed for \${currentUser}/\${repo}, retrying...\`);
              await delay(5000);
            }
          }
        }
        
        // å»¶è¿Ÿå‡½æ•°
        function delay(ms) {
          return new Promise(resolve => setTimeout(resolve, ms));
        }
        
        // åˆ†ç»„å¤„ç†
        function chunk(array, size) {
          const chunks = [];
          for (let i = 0; i < array.length; i += size) {
            chunks.push(array.slice(i, i + size));
          }
          return chunks;
        }
        
        // ä¸»å‡½æ•°
        async function main() {
          try {
            console.log('ğŸš€ å¼€å§‹æ‰¹é‡forkæ“ä½œ...');
            
            let pluginJson;
            
            // æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦ä¸‹è½½æœ€æ–°çš„ plugins.json
            if (process.env.DOWNLOAD_LATEST === 'true') {
              console.log('ğŸ“¥ ä¸‹è½½æœ€æ–°çš„ plugins.json...');
              try {
                pluginJson = await downloadLatestPluginsJson();
                console.log('âœ… æˆåŠŸä¸‹è½½æœ€æ–°çš„ plugins.json');
              } catch (error) {
                console.log(\`âš ï¸  ä¸‹è½½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: \${error.message}\`);
                // å›é€€åˆ°æœ¬åœ°æ–‡ä»¶
                const pluginJsonPath = path.join(process.cwd(), 'plugins.json');
                if (!fs.existsSync(pluginJsonPath)) {
                  throw new Error('plugins.json æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä¸”æ— æ³•ä¸‹è½½æœ€æ–°ç‰ˆæœ¬');
                }
                pluginJson = await fs.readJson(pluginJsonPath);
              }
            } else {
              // ä½¿ç”¨æœ¬åœ°æ–‡ä»¶
              console.log('ğŸ“ ä½¿ç”¨æœ¬åœ° plugins.json...');
              const pluginJsonPath = path.join(process.cwd(), 'plugins.json');
              if (!fs.existsSync(pluginJsonPath)) {
                throw new Error('plugins.json æ–‡ä»¶ä¸å­˜åœ¨');
              }
              pluginJson = await fs.readJson(pluginJsonPath);
            }
            const repoInfos = extractRepoUrls(pluginJson);
            
            console.log(\`ğŸ“‹ æ‰¾åˆ° \${repoInfos.length} ä¸ªæ’ä»¶ä»“åº“:\`);
            repoInfos.forEach((info, index) => {
              console.log(\`  \${index + 1}. \${info.name} -> \${info.repo}\`);
              if (info.description) {
                console.log(\`     æè¿°: \${info.description}\`);
              }
            });
            
            if (repoInfos.length === 0) {
              console.log('âš ï¸  æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„GitHubä»“åº“');
              return;
            }
            
            // è·å–å½“å‰ç”¨æˆ·
            const currentUser = await getCurrentUser();
            console.log(\`ğŸ‘¤ å½“å‰ç”¨æˆ·: \${currentUser}\`);
            
            // åˆ†ç»„å¤„ç†ï¼ˆæ¯ç»„10ä¸ªï¼‰
            const groups = chunk(repoInfos, 10);
            console.log(\`ğŸ“¦ åˆ†ä¸º \${groups.length} ç»„å¤„ç†\`);
            
            let totalProcessed = 0;
            let totalForked = 0;
            let totalSynced = 0;
            let totalErrors = 0;
            
            for (let i = 0; i < groups.length; i++) {
              const group = groups[i];
              console.log(\`\\nğŸ“¦ å¤„ç†ç¬¬ \${i + 1}/\${groups.length} ç»„ (å…± \${group.length} ä¸ªä»“åº“):\`);
              
              // å¹¶å‘å¤„ç†å½“å‰ç»„
              const groupPromises = group.map(async (repoInfo) => {
                const [owner, repo] = repoInfo.repo.split('/');
                
                try {
                  // æ£€æŸ¥æ˜¯å¦å·²ç»forkè¿‡
                  const isForked = await checkIfForked(owner, repo, currentUser);
                  
                  if (isForked) {
                    console.log(\`ğŸ”„ æ’ä»¶ \${repoInfo.name} (\${owner}/\${repo}) å·²ç»forkè¿‡ï¼Œæ‰§è¡ŒåŒæ­¥...\`);
                    await syncFork(owner, repo, currentUser);
                    totalSynced++;
                  } else {
                    console.log(\`ğŸ†• æ’ä»¶ \${repoInfo.name} (\${owner}/\${repo}) æœªforkï¼Œæ‰§è¡Œfork...\`);
                    await forkRepository(owner, repo, currentUser);
                    totalForked++;
                  }
                  
                  totalProcessed++;
                } catch (error) {
                  console.error(\`âŒ å¤„ç†æ’ä»¶ \${repoInfo.name} (\${owner}/\${repo}) æ—¶å‡ºé”™: \${error.message}\`);
                  totalErrors++;
                }
              });
              
              await Promise.all(groupPromises);
              
              // å¦‚æœä¸æ˜¯æœ€åä¸€ç»„ï¼Œç­‰å¾…30ç§’
              if (i < groups.length - 1) {
                console.log('â±ï¸  ç­‰å¾…30ç§’åå¤„ç†ä¸‹ä¸€ç»„...');
                await delay(30000);
              }
            }
            
            console.log(\`\\nâœ… æ‰¹é‡forkæ“ä½œå®Œæˆï¼\`);
            console.log(\`ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:\`);
            console.log(\`  - æ€»å…±å¤„ç†: \${totalProcessed} ä¸ªæ’ä»¶ä»“åº“\`);
            console.log(\`  - æ–°fork: \${totalForked} ä¸ª\`);
            console.log(\`  - åŒæ­¥æ›´æ–°: \${totalSynced} ä¸ª\`);
            console.log(\`  - é”™è¯¯: \${totalErrors} ä¸ª\`);
            
            if (totalErrors > 0) {
              console.log(\`\\nâš ï¸  æœ‰ \${totalErrors} ä¸ªä»“åº“å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—\`);
            }
            
          } catch (error) {
            console.error('âŒ æ‰¹é‡forkæ“ä½œå¤±è´¥:', error.message);
            process.exit(1);
          }
        }
        
        // è¿è¡Œä¸»å‡½æ•°
        main();
        "
